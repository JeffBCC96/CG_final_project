{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capsnet model for human brain dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "read_counts = pd.read_table('/Users/marong/Dropbox/CG_project/Capsnet/6k/data6k_norm_counts_all.txt')\n",
    "cell_labels = pd.read_table('/Users/marong/Dropbox/CG_project/Capsnet/6k/data6k_celltypes.txt')\n",
    "read_counts_2 = pd.read_table('/Users/marong/Dropbox/CG_project/Capsnet/8k/data8k_norm_counts_all.txt')\n",
    "cell_labels_2 = pd.read_table('/Users/marong/Dropbox/CG_project/Capsnet/8k/data8k_celltypes.txt')\n",
    "\n",
    "count_6k = read_counts.iloc[:,1:]\n",
    "count_8k = read_counts_2.iloc[:,1:]\n",
    "\n",
    "all_data = count_6k.append(count_8k, sort=False)\n",
    "data_processed = all_data.loc[:,count_6k.columns]\n",
    "data_processed = data_processed.fillna(0)\n",
    "all_labels = list(cell_labels['Celltype']) + list (cell_labels_2['Celltype'])\n",
    "\n",
    "x  = data_processed\n",
    "y = all_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract info and reorganize the data\n",
    "x = torch.FloatTensor(x.values)\n",
    "y_labels = set(y)\n",
    "y_label_dict = dict(zip(y_labels, range(0, len(y_labels))))\n",
    "y_index = [y_label_dict[i] for i in y]\n",
    "\n",
    "np.random.seed(888)\n",
    "rand_index = np.random.permutation(len(y_index))\n",
    "x_data = x[torch.tensor(rand_index),:]\n",
    "y_data = [y_index[i] for i in rand_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12957, 13137])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "X_train = X_train.view(-1, 1,13137)\n",
    "X_test = X_test.view(-1, 1,13137)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10365, 1, 13137])\n",
      "10365\n",
      "torch.Size([1296, 1, 13137])\n",
      "1296\n",
      "torch.Size([1296, 1, 13137])\n",
      "1296\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(len(y_train))\n",
    "print(X_valid.shape)\n",
    "print(len(y_valid))\n",
    "print(X_test.shape)\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([100, 1, 13137])\n",
      "Sample input: \n",
      " tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.7181,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])\n",
      "\n",
      "Sample label size:  torch.Size([100])\n",
      "Sample label: \n",
      " tensor([3, 5, 2, 0, 0, 0, 4, 4, 4, 5, 2, 4, 2, 0, 0, 5, 2, 5, 4, 4, 4, 0, 4, 0,\n",
      "        4, 5, 2, 0, 5, 0, 4, 1, 2, 5, 4, 0, 4, 2, 0, 0, 5, 0, 5, 4, 2, 0, 0, 0,\n",
      "        5, 2, 0, 4, 4, 0, 5, 5, 2, 0, 0, 4, 2, 0, 2, 2, 2, 0, 2, 4, 5, 4, 3, 0,\n",
      "        4, 0, 0, 5, 2, 5, 2, 4, 5, 4, 5, 2, 2, 2, 4, 2, 2, 2, 5, 0, 2, 5, 5, 4,\n",
      "        0, 4, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# create \n",
    "train_data = TensorDataset(X_train, torch.LongTensor(y_train))\n",
    "valid_data = TensorDataset(X_valid, torch.LongTensor(y_valid))\n",
    "test_data = TensorDataset(X_test, torch.LongTensor(y_test))\n",
    "\n",
    "# dataloaders \n",
    "batch_size = 100\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size,drop_last =True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last = True)\n",
    "\n",
    "\n",
    "# check one of the batches\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) \n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) \n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=8, in_feature = 13137, out_feature=256):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "\n",
    "        self.capsules = nn.ModuleList([\n",
    "            nn.Linear(in_feature, out_feature)\n",
    "            for _ in range(num_capsules)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = [capsule(x) for capsule in self.capsules]\n",
    "        u = torch.stack(u, dim=1)\n",
    "        u = u.view(x.size(0), 256, -1)\n",
    "\n",
    "        return self.squash(u)\n",
    "\n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm * input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n",
    "    \n",
    "# output 100 , 256 , 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=8, num_routes=256, in_channels=8, out_channels=16):\n",
    "        super(DigitCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        self.W = nn.Parameter(torch.randn(1, num_routes, num_capsules, out_channels, in_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        num_iterations = 3\n",
    "\n",
    "        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n",
    "        W = torch.cat([self.W] * batch_size, dim=0)\n",
    "        u_hat = torch.matmul(W, x)\n",
    "        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1))\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            c_ij = F.softmax(b_ij)\n",
    "            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n",
    "\n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
    "            v_j = self.squash(s_j)\n",
    "            \n",
    "            if iteration < num_iterations - 1:\n",
    "                a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1))\n",
    "                b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)\n",
    "\n",
    "        return v_j.squeeze(1)\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n",
    "    \n",
    "    \n",
    "# output torch.Size([100, 8, 16, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.reconstraction_layers = nn.Sequential(\n",
    "            nn.Linear(16 * 8, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 13137),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, data):\n",
    "        classes = torch.sqrt((x ** 2).sum(2))\n",
    "        classes = F.softmax(classes)\n",
    "        \n",
    "        _, max_length_indices = classes.max(dim=1)\n",
    "        masked = Variable(torch.sparse.torch.eye(8))\n",
    "        masked = masked.index_select(dim=0, index=max_length_indices.squeeze(1).data)\n",
    "        reconstructions = self.reconstraction_layers((x * masked[:, :, None, None]).view(x.size(0), -1))\n",
    "        reconstructions = reconstructions.view(-1, 1, 13137)\n",
    "        \n",
    "        return reconstructions, masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.primary_capsules = PrimaryCaps()\n",
    "        self.digit_capsules = DigitCaps()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, data):\n",
    "        output = self.digit_capsules(\n",
    "            self.primary_capsules(data))\n",
    "\n",
    "        reconstructions, masked = self.decoder(output, data)\n",
    "        return output, reconstructions, masked\n",
    "\n",
    "    def loss(self, data, x, target, reconstructions):\n",
    "        return self.margin_loss(x, target) + self.reconstruction_loss(data, reconstructions)\n",
    "\n",
    "    def margin_loss(self, x, labels, size_average=True):\n",
    "        batch_size = x.size(0)\n",
    "        v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n",
    "\n",
    "        left = F.relu(0.9 - v_c).view(batch_size, -1)\n",
    "        right = F.relu(v_c - 0.1).view(batch_size, -1)\n",
    "\n",
    "        loss = labels * left + 0.5 * (1.0 - labels) * right\n",
    "        loss = loss.sum(dim=1).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def reconstruction_loss(self, data, reconstructions):\n",
    "        loss = self.mse_loss(reconstructions.view(reconstructions.size(0), -1), data.view(reconstructions.size(0), -1))\n",
    "        return loss * 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "capsule_net = CapsNet()\n",
    "optimizer = Adam(capsule_net.parameters())\n",
    "\n",
    "batch_size = 100\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train accuracy: 0.2\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "train accuracy: 0.64\n",
      "tensor(0.5791)\n",
      "Valid accuracy: 0.77\n",
      "tensor(0.3824)\n",
      "0\n",
      "train accuracy: 0.72\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "train accuracy: 0.85\n",
      "tensor(0.2910)\n",
      "Valid accuracy: 0.86\n",
      "tensor(0.2347)\n",
      "0\n",
      "train accuracy: 0.93\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "train accuracy: 0.91\n",
      "tensor(0.1442)\n",
      "Valid accuracy: 0.89\n",
      "tensor(0.1720)\n",
      "0\n",
      "train accuracy: 0.93\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "train accuracy: 0.96\n",
      "tensor(0.0725)\n",
      "Valid accuracy: 0.95\n",
      "tensor(0.1446)\n",
      "0\n",
      "train accuracy: 1.0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "train accuracy: 0.96\n",
      "tensor(0.0392)\n",
      "Valid accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "early_stop_time = 0\n",
    "max_accuracy = -1\n",
    "early_stop = False\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    capsule_net.train()\n",
    "    train_loss = 0\n",
    "    for batch_id, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        target = torch.sparse.torch.eye(8).index_select(dim=0, index=target)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        data = data.float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output, reconstructions, masked = capsule_net(data)\n",
    "        loss = capsule_net.loss(data, output, target, reconstructions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data\n",
    "        if batch_id %10 ==0: \n",
    "            print(batch_id)\n",
    "\n",
    "        if batch_id % 100 == 0:\n",
    "            print(\"train accuracy:\", sum(np.argmax(masked.data.cpu().numpy(), 1) ==\n",
    "                                         np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size))\n",
    "\n",
    "    print(train_loss / len(train_loader))\n",
    "\n",
    "    # Valid Dataset\n",
    "    capsule_net.eval()\n",
    "    valid_loss = 0\n",
    "    for batch_id, (data, target) in enumerate(valid_loader):\n",
    "\n",
    "        target = torch.sparse.torch.eye(8).index_select(dim=0, index=target)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        data = data.float()\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        output, reconstructions, masked = capsule_net(data)\n",
    "        loss = capsule_net.loss(data, output, target, reconstructions)\n",
    "\n",
    "        valid_loss += loss.data\n",
    "\n",
    "        if batch_id % 100 == 0:\n",
    "            print(\"Valid accuracy:\", sum(np.argmax(masked.data.cpu().numpy(), 1) ==\n",
    "                                        np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size))\n",
    "            \n",
    "            val_accu = sum(np.argmax(masked.data.cpu().numpy(), 1) == np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size)\n",
    "            if val_accu >= max_accuracy:\n",
    "                max_accuracy = val_accu\n",
    "                #early_stop_time = 0\n",
    "            else:\n",
    "                early_stop = True\n",
    "                #early_stop_time += 1\n",
    "\n",
    "    if early_stop == True:\n",
    "        break\n",
    "\n",
    "    print(valid_loss / len(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(capsule_net.state_dict(), '/Users/marong/Dropbox/CG_project/Capsnet/capsnet_fc_PBMC.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.91\n",
      "Test accuracy: 0.9\n",
      "Test accuracy: 0.85\n",
      "Test accuracy: 0.92\n",
      "Test accuracy: 0.9\n",
      "Test accuracy: 0.91\n",
      "Test accuracy: 0.92\n",
      "Test accuracy: 0.95\n",
      "Test accuracy: 0.89\n",
      "Test accuracy: 0.92\n",
      "Test accuracy: 0.88\n",
      "Test accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "capsule_net.eval()\n",
    "pred_results = []\n",
    "test_loss = 0\n",
    "accuracy = []\n",
    "y_pred, y_true = [], []\n",
    "for batch_id, (data, target) in enumerate(test_loader):\n",
    "    target = torch.sparse.torch.eye(8).index_select(dim=0, index=target)\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    data = data.float()\n",
    "    output, reconstructions, masked = capsule_net(data)\n",
    "    pred_results.append((output,reconstructions,masked))\n",
    "    loss = capsule_net.loss(data, output, target, reconstructions)\n",
    "\n",
    "    test_loss += loss.data\n",
    "    yp= np.argmax(masked.data.cpu().numpy(), 1)\n",
    "    yt = np.argmax(target.data.cpu().numpy(), 1)\n",
    "    y_pred = y_pred + list(yp)\n",
    "    y_true = y_true + list(yt)\n",
    "\n",
    "    \n",
    "    acc =sum(np.argmax(masked.data.cpu().numpy(), 1) == np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size)\n",
    "    \n",
    "    print(\"Test accuracy:\", acc)\n",
    "    accuracy.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9066666666666667\n",
      "(0.9107267417696119, 0.9066666666666666, 0.9075402269082887, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "print(np.mean(accuracy))\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
